{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "train_data = np.loadtxt('./hw4_train.dat.txt')\n",
    "test_data = np.loadtxt('./hw4_test.dat.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.568304   0.568283  ]\n",
      " [1.         0.310968   0.310956  ]\n",
      " [1.         0.103376   0.103373  ]\n",
      " [1.         0.0531882  0.053218  ]\n",
      " [1.         0.97006    0.970064  ]\n",
      " [1.         0.0941873  0.0941707 ]\n",
      " [1.         0.655902   0.655892  ]\n",
      " [1.         0.370821   0.370839  ]\n",
      " [1.         0.558482   0.558476  ]\n",
      " [1.         0.849389   0.849383  ]\n",
      " [1.         0.796038   0.796051  ]\n",
      " [1.         0.723246   0.723252  ]\n",
      " [1.         0.571236   0.571254  ]\n",
      " [1.         0.385144   0.38512   ]\n",
      " [1.         0.877176   0.877168  ]\n",
      " [1.         0.74655    0.746552  ]\n",
      " [1.         0.0676164  0.0676087 ]\n",
      " [1.         0.0412524  0.0412649 ]\n",
      " [1.         0.851637   0.851661  ]\n",
      " [1.         0.586989   0.58698   ]\n",
      " [1.         0.661014   0.660994  ]\n",
      " [1.         0.587988   0.587968  ]\n",
      " [1.         0.257615   0.257628  ]\n",
      " [1.         0.680505   0.680485  ]\n",
      " [1.         0.895242   0.895257  ]\n",
      " [1.         0.381124   0.381139  ]\n",
      " [1.         0.314332   0.31433   ]\n",
      " [1.         0.157744   0.157747  ]\n",
      " [1.         0.670923   0.670925  ]\n",
      " [1.         0.531716   0.531736  ]\n",
      " [1.         0.810956   0.810938  ]\n",
      " [1.         0.514937   0.51493   ]\n",
      " [1.         0.188567   0.188587  ]\n",
      " [1.         0.778528   0.778527  ]\n",
      " [1.         0.904966   0.904955  ]\n",
      " [1.         0.563699   0.563708  ]\n",
      " [1.         0.599768   0.59978   ]\n",
      " [1.         0.619909   0.619928  ]\n",
      " [1.         0.650556   0.650556  ]\n",
      " [1.         0.131949   0.131967  ]\n",
      " [1.         0.251546   0.251546  ]\n",
      " [1.         0.690874   0.690863  ]\n",
      " [1.         0.381249   0.381284  ]\n",
      " [1.         0.559231   0.559232  ]\n",
      " [1.         0.197361   0.197367  ]\n",
      " [1.         0.784776   0.784781  ]\n",
      " [1.         0.620494   0.620499  ]\n",
      " [1.         0.229646   0.229647  ]\n",
      " [1.         0.0891466  0.0891438 ]\n",
      " [1.         0.981857   0.981861  ]\n",
      " [1.         0.64711    0.647102  ]\n",
      " [1.         0.725596   0.725592  ]\n",
      " [1.         0.614771   0.614764  ]\n",
      " [1.         0.976315   0.976321  ]\n",
      " [1.         0.250716   0.250708  ]\n",
      " [1.         0.281071   0.281096  ]\n",
      " [1.         0.550196   0.550187  ]\n",
      " [1.         0.955756   0.955751  ]\n",
      " [1.         0.251821   0.251838  ]\n",
      " [1.         0.538196   0.538183  ]\n",
      " [1.         0.58285    0.582836  ]\n",
      " [1.         0.48367    0.48368   ]\n",
      " [1.         0.481451   0.481471  ]\n",
      " [1.         0.291576   0.291561  ]\n",
      " [1.         0.181592   0.181596  ]\n",
      " [1.         0.232746   0.232759  ]\n",
      " [1.         0.488322   0.488349  ]\n",
      " [1.         0.664499   0.664487  ]\n",
      " [1.         0.0420094  0.0420475 ]\n",
      " [1.         0.950521   0.950524  ]\n",
      " [1.         0.445707   0.445706  ]\n",
      " [1.         0.430385   0.430396  ]\n",
      " [1.         0.747574   0.747583  ]\n",
      " [1.         0.245047   0.245078  ]\n",
      " [1.         0.742838   0.742833  ]\n",
      " [1.         0.284625   0.284627  ]\n",
      " [1.         0.0613909  0.061374  ]\n",
      " [1.         0.612767   0.612754  ]\n",
      " [1.         0.378545   0.378555  ]\n",
      " [1.         0.818764   0.818763  ]\n",
      " [1.         0.0507026  0.0507136 ]\n",
      " [1.         0.882725   0.882731  ]\n",
      " [1.         0.0810847  0.0810796 ]\n",
      " [1.         0.836278   0.836279  ]\n",
      " [1.         0.696709   0.696695  ]\n",
      " [1.         0.603346   0.603334  ]\n",
      " [1.         0.513718   0.513712  ]\n",
      " [1.         0.247789   0.247802  ]\n",
      " [1.         0.704221   0.704213  ]\n",
      " [1.         0.546723   0.546724  ]\n",
      " [1.         0.881583   0.881592  ]\n",
      " [1.         0.13456    0.134545  ]\n",
      " [1.         0.86883    0.868815  ]\n",
      " [1.         0.980909   0.980887  ]\n",
      " [1.         0.369986   0.369986  ]\n",
      " [1.         0.194455   0.194457  ]\n",
      " [1.         0.483858   0.483875  ]\n",
      " [1.         0.43807    0.43808   ]\n",
      " [1.         0.159602   0.159592  ]\n",
      " [1.         0.923499   0.923504  ]\n",
      " [1.         0.419902   0.419906  ]\n",
      " [1.         0.659252   0.659271  ]\n",
      " [1.         0.419546   0.419546  ]\n",
      " [1.         0.935494   0.935512  ]\n",
      " [1.         0.712397   0.71239   ]\n",
      " [1.         0.952567   0.952549  ]\n",
      " [1.         0.915359   0.915379  ]\n",
      " [1.         0.182693   0.182675  ]\n",
      " [1.         0.668527   0.668522  ]\n",
      " [1.         0.0965221  0.0965266 ]\n",
      " [1.         0.984174   0.984197  ]\n",
      " [1.         0.7437     0.743702  ]\n",
      " [1.         0.213357   0.213341  ]\n",
      " [1.         0.617402   0.617386  ]\n",
      " [1.         0.335604   0.335604  ]\n",
      " [1.         0.632581   0.632597  ]\n",
      " [1.         0.515744   0.515757  ]\n",
      " [1.         0.786921   0.786912  ]\n",
      " [1.         0.502608   0.502599  ]\n",
      " [1.         0.164538   0.164537  ]\n",
      " [1.         0.507454   0.507469  ]\n",
      " [1.         0.822809   0.822806  ]\n",
      " [1.         0.42883    0.428821  ]\n",
      " [1.         0.157678   0.157693  ]\n",
      " [1.         0.674884   0.674896  ]\n",
      " [1.         0.276618   0.276622  ]\n",
      " [1.         0.374795   0.374795  ]\n",
      " [1.         0.396781   0.396815  ]\n",
      " [1.         0.132116   0.132101  ]\n",
      " [1.         0.966203   0.966249  ]\n",
      " [1.         0.961164   0.961159  ]\n",
      " [1.         0.0140044  0.014014  ]\n",
      " [1.         0.509361   0.509379  ]\n",
      " [1.         0.195082   0.195097  ]\n",
      " [1.         0.853012   0.853012  ]\n",
      " [1.         0.852883   0.852896  ]\n",
      " [1.         0.574279   0.574282  ]\n",
      " [1.         0.316965   0.316939  ]\n",
      " [1.         0.386753   0.386761  ]\n",
      " [1.         0.764792   0.764815  ]\n",
      " [1.         0.680442   0.680428  ]\n",
      " [1.         0.125299   0.125304  ]\n",
      " [1.         0.619824   0.619818  ]\n",
      " [1.         0.687672   0.687662  ]\n",
      " [1.         0.760271   0.760289  ]\n",
      " [1.         0.227148   0.22713   ]\n",
      " [1.         0.224288   0.224295  ]\n",
      " [1.         0.0150326  0.0150352 ]\n",
      " [1.         0.585322   0.585314  ]\n",
      " [1.         0.732755   0.732777  ]\n",
      " [1.         0.864553   0.864569  ]\n",
      " [1.         0.0788415  0.0788569 ]\n",
      " [1.         0.4326     0.432602  ]\n",
      " [1.         0.804816   0.804801  ]\n",
      " [1.         0.50957    0.509589  ]\n",
      " [1.         0.405003   0.404988  ]\n",
      " [1.         0.465702   0.465691  ]\n",
      " [1.         0.368576   0.368574  ]\n",
      " [1.         0.56202    0.562033  ]\n",
      " [1.         0.552361   0.552356  ]\n",
      " [1.         0.18263    0.182606  ]\n",
      " [1.         0.672912   0.672906  ]\n",
      " [1.         0.642397   0.642413  ]\n",
      " [1.         0.816308   0.816316  ]\n",
      " [1.         0.264986   0.264978  ]\n",
      " [1.         0.799168   0.799179  ]\n",
      " [1.         0.311442   0.311432  ]\n",
      " [1.         0.715291   0.715278  ]\n",
      " [1.         0.913262   0.913265  ]\n",
      " [1.         0.703566   0.70358   ]\n",
      " [1.         0.0868818  0.0868856 ]\n",
      " [1.         0.507828   0.507835  ]\n",
      " [1.         0.77619    0.776196  ]\n",
      " [1.         0.503254   0.503257  ]\n",
      " [1.         0.0585257  0.0585251 ]\n",
      " [1.         0.668003   0.667995  ]\n",
      " [1.         0.409675   0.409686  ]\n",
      " [1.         0.00104673 0.00105247]\n",
      " [1.         0.6743     0.674268  ]\n",
      " [1.         0.461383   0.461378  ]\n",
      " [1.         0.957667   0.957677  ]\n",
      " [1.         0.386593   0.386566  ]\n",
      " [1.         0.260177   0.260171  ]\n",
      " [1.         0.208071   0.208076  ]\n",
      " [1.         0.634661   0.634646  ]\n",
      " [1.         0.354351   0.354351  ]\n",
      " [1.         0.135384   0.135381  ]\n",
      " [1.         0.216718   0.216748  ]\n",
      " [1.         0.606084   0.606096  ]\n",
      " [1.         0.443809   0.443801  ]\n",
      " [1.         0.480428   0.480418  ]\n",
      " [1.         0.886987   0.886995  ]\n",
      " [1.         0.0126171  0.012603  ]\n",
      " [1.         0.578502   0.578495  ]\n",
      " [1.         0.0664441  0.0664438 ]\n",
      " [1.         0.292442   0.292432  ]\n",
      " [1.         0.487013   0.487008  ]\n",
      " [1.         0.176237   0.176234  ]\n",
      " [1.         0.496052   0.496044  ]\n",
      " [1.         0.62186    0.621853  ]]\n",
      "(200, 3) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "def sign(x):\n",
    "    r = np.ones(x.shape)\n",
    "    for idx, value in enumerate(x):\n",
    "        r[idx] = (-1 if value < 0 else 1)\n",
    "    return r\n",
    "\n",
    "def get_dataset(x):\n",
    "    X = x[:, 0:x.shape[1]-1]\n",
    "    y = x[:, x.shape[1]-1: x.shape[1]]\n",
    "    X = np.concatenate((np.ones((X.shape[0],1)), X), axis=1)\n",
    "    return X,y\n",
    "\n",
    "X_train, y_train = get_dataset(train_data)\n",
    "X_test, y_test = get_dataset(test_data)\n",
    "\n",
    "print(X_train)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_reg(x, y, lambda_reg):\n",
    "    # linear regression\n",
    "    # w_reg = (((x_tran*x + lambda * I)_inver)* x_tran) * y\n",
    "    #       (x_tran*x + lambda * I) = first_part\n",
    "    #       first_part_inver * x_tran = second_part\n",
    "    # w_reg = second_part * y\n",
    "    first_part = np.dot(np.transpose(x), x) + lambda_reg * np.eye(x.shape[1])\n",
    "    first_part_inver = np.linalg.inv(first_part)\n",
    "    second_part = np.dot(first_part_inver, np.transpose(x))\n",
    "    w_reg = np.dot(second_part, y)\n",
    "    return w_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(x, y, w):\n",
    "    return float(np.sum(sign(np.dot(x, w)) != y))/x.shape[0]\n",
    "\n",
    "def q_13(lambda_reg = 10):\n",
    "    w = get_w_reg(X_train, y_train, lambda_reg)\n",
    "    e_in = get_error(X_train, y_train, w)\n",
    "    e_out = get_error(X_test, y_test, w)\n",
    "    return (e_in, e_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_1415(is_q_15 = False):\n",
    "    e_in_min = 100\n",
    "    e_out_min = e_in_min\n",
    "    lambda_reg_min = 100\n",
    "    for log_lambda in range(2, -11, -1):\n",
    "        lambda_reg = 10 ** log_lambda\n",
    "        e_in, e_out = q_13(lambda_reg)\n",
    "        if is_q_15 is False:\n",
    "            if e_in < e_in_min:\n",
    "                e_in_min = e_in\n",
    "                e_out_min = e_out\n",
    "                lambda_reg_min = log_lambda\n",
    "        else:\n",
    "            if e_out < e_out_min:\n",
    "                e_in_min = e_in\n",
    "                e_out_min = e_out\n",
    "                lambda_reg_min = log_lambda\n",
    "        \n",
    "    return e_in_min, e_out_min, lambda_reg_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_161718(is_q_17=False, is_q_18 = False):\n",
    "    n_train = 120\n",
    "    _X_train = X_train[:n_train]\n",
    "    _y_train = y_train[:n_train]\n",
    "    if is_q_18:\n",
    "        _X_train = X_train\n",
    "        _y_train = y_train\n",
    "    X_val = X_train[n_train:]\n",
    "    y_val = y_train[n_train:]\n",
    "    e_in_min = 100\n",
    "    e_val_min = e_in_min\n",
    "    e_out_min = e_val_min\n",
    "    lambda_reg_min = e_out_min\n",
    "    for log_lambda in range(2, -11, -1):\n",
    "        lambda_reg = 10 ** log_lambda\n",
    "        w_reg = get_w_reg(_X_train, _y_train, lambda_reg)\n",
    "        e_in = get_error(X_train[:n_train], y_train[:n_train], w_reg)\n",
    "        e_val = get_error(X_val, y_val, w_reg)\n",
    "        e_out = get_error(X_test, y_test, w_reg)\n",
    "        if is_q_17 is False:\n",
    "            if e_in < e_in_min:\n",
    "                e_in_min = e_in\n",
    "                e_val_min = e_val\n",
    "                e_out_min = e_out\n",
    "                lambda_reg_min = log_lambda\n",
    "        else:\n",
    "            if e_val < e_val_min:\n",
    "                e_in_min = e_in\n",
    "                e_val_min = e_val\n",
    "                e_out_min = e_out\n",
    "                lambda_reg_min = log_lambda\n",
    "    return e_in_min, e_val_min, e_out_min, lambda_reg_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_dataset(x, y, k):\n",
    "    try:\n",
    "        if k > 1:\n",
    "            n_dataset = int(x.shape[0]/k)\n",
    "            x_result = []\n",
    "            y_result = []\n",
    "            for idx in range(k):\n",
    "                r_init = n_dataset * idx\n",
    "                r_fin = r_init + n_dataset\n",
    "                x_result.append(x[r_init: r_fin])\n",
    "                y_result.append(y[r_init: r_fin])\n",
    "            return x_result, y_result\n",
    "        else:\n",
    "            raise ValueError(\"k must greater that 1\")\n",
    "    except ValueError:\n",
    "        raise\n",
    "\n",
    "def q_1920():\n",
    "    # k cross validatioin\n",
    "    x_datasets, y_datasets = get_cross_dataset(X_train, y_train, 5)\n",
    "    \n",
    "    e_val_min = 100000\n",
    "    lambda_reg_min = e_val_min\n",
    "    \n",
    "    for log_lambda in range(2, -11, -1):\n",
    "        lambda_reg = 10 ** log_lambda\n",
    "        current_val_idx = 0\n",
    "        e_val = 0\n",
    "        while current_val_idx < len(x_datasets):\n",
    "            _x_train = None\n",
    "            _y_train = None\n",
    "            _x_val = None\n",
    "            _y_val = None\n",
    "            for idx, value in enumerate(x_datasets):\n",
    "                if idx != current_val_idx:\n",
    "                    # 拿來訓練\n",
    "                    if _x_train is None:\n",
    "                        _x_train = value\n",
    "                        _y_train = y_datasets[idx]\n",
    "                    else:\n",
    "                        _x_train = np.vstack((_x_train, value))\n",
    "                        _y_train = np.vstack((_y_train, y_datasets[idx]))\n",
    "                else:\n",
    "                    # 拿來validation\n",
    "                    _x_val = value\n",
    "                    _y_val = y_datasets[idx]\n",
    "            w_reg = get_w_reg(_x_train, _y_train, lambda_reg)\n",
    "            # 算每一輪e_val的加總\n",
    "            e_val += get_error(_x_val, _y_val, w_reg)  \n",
    "            current_val_idx += 1\n",
    "        # 然後再取平均\n",
    "        e_val_avg = e_val/len(x_datasets)\n",
    "        \n",
    "        # 找出最小的\n",
    "        if e_val_avg < e_val_min:\n",
    "            e_val_min = e_val_avg\n",
    "            lambda_reg_min = log_lambda\n",
    "    return e_val_min, lambda_reg_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q13\n",
      "0.05 0.045\n",
      "\n",
      "q14\n",
      "-8 0.015 0.02\n",
      "\n",
      "q15\n",
      "-7 0.03 0.015\n"
     ]
    }
   ],
   "source": [
    "print(\"q13\")\n",
    "e_in, e_out = q_13()\n",
    "print(e_in, e_out)\n",
    "# q13的答案 e_in = 0.05, e_out = 0.045\n",
    "print(\"\")\n",
    "print(\"q14\")\n",
    "e_in, e_out, lambda_reg = q_1415()\n",
    "print(lambda_reg, e_in, e_out)\n",
    "# q_14 -> lamba = -8 e_in = 0.015, e_out = 0.02\n",
    "print(\"\")\n",
    "print(\"q15\")\n",
    "e_in, e_out, lambda_reg = q_1415(True)\n",
    "print(lambda_reg, e_in, e_out)\n",
    "# q_15 -> lamba = -7 e_in =. 0.03, e_out = 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q16\n",
      "-8 0.0 0.05 0.025\n",
      "\n",
      "q17\n",
      "0 0.03333333333333333 0.0375 0.028\n",
      "\n",
      "q18\n",
      "0.03333333333333333 0.016\n"
     ]
    }
   ],
   "source": [
    "print(\"q16\")\n",
    "e_in, e_val, e_out, lambda_reg = q_161718()\n",
    "print(lambda_reg, e_in, e_val, e_out)\n",
    "# q_16 -> lamba = -8 e_in =. 0.0, e_val = 0.05, e_out = 0.025\n",
    "print(\"\")\n",
    "print(\"q17\")\n",
    "e_in, e_val, e_out, lambda_reg = q_161718(True)\n",
    "print(lambda_reg, e_in, e_val, e_out)\n",
    "# q_17 -> lamba = 0 e_in =. 0.03, e_val = 0.038, e_out = 0.028\n",
    "print(\"\")\n",
    "print(\"q18\")\n",
    "e_in, e_val, e_out, lambda_reg = q_161718(True, True)\n",
    "print(e_in, e_out)\n",
    "# q_18 -> e_in =. 0.035, e_out = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q19\n",
      "-8 0.03\n",
      "\n",
      "q20\n",
      "0.015 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"q19\")\n",
    "e_cv, lambda_reg = q_1920()\n",
    "print(lambda_reg, e_cv)\n",
    "# q_19 -> lambda = -8, e_cv = 0.03\n",
    "print(\"\")\n",
    "print(\"q20\")\n",
    "e_in, e_out = q_13(10**lambda_reg)\n",
    "print(e_in, e_out)\n",
    "# q_20 -> e_in = 0.015, e_out = 0.02"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
